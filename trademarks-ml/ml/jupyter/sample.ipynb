{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import sklearn.metrics as metrics\n",
    "import os\n",
    "import logging\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import h5py\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/1381-design-codes.txt\", \"r\") as file:\n",
    "    codes = file.read().splitlines()\n",
    "    codes_new = [item[0:2]+\".\"+item[2:4]+\".\"+item[4:6] for item in codes]\n",
    "    with open(\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/1381-design-codes_new.txt\", \"w\") as file2:\n",
    "        for code in codes_new:\n",
    "            file2.write(\"%s\\n\"%code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/serialextractor-no-prefix/designcode-counts.txt\", \"r\") as file:\n",
    "    lines = file.read().splitlines()\n",
    "design_codes = []\n",
    "counts = []\n",
    "for line in lines:\n",
    "    design_codes.append(line.split(\" - \")[0])\n",
    "    counts.append(int(line.split(\" - \")[1]))\n",
    "print (str(len(design_codes)))\n",
    "print (str(len(counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = 0\n",
    "design_codes_with_min_support = []\n",
    "for index, count in enumerate(counts):\n",
    "    if count >= 500:\n",
    "        print(\"{} - {}\".format(design_codes[index], count))\n",
    "        total_count = total_count+1\n",
    "        design_codes_with_min_support.append(design_codes[index])\n",
    "print(\"Total count - \"+str(total_count))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/818-design-codes/818-designCodes.txt\", \"w\") as file:\n",
    "    for code in design_codes_with_min_support:\n",
    "        file.write(\"%s\\n\"%code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/818-design-codes/818-designCodes.txt\", \"r\") as file:\n",
    "    codes_818 = file.read().splitlines()\n",
    "\n",
    "with open(\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/serialextractor-no-prefix/1003dsc.txt\", \"r\") as file:\n",
    "    girish_codes = file.read().splitlines()\n",
    "    girish_codes = [item[0:2]+\".\"+item[2:4]+\".\"+item[4:6] for item in girish_codes]\n",
    "\n",
    "print(str(len(girish_codes)))\n",
    "\n",
    "girish_set = set(girish_codes)\n",
    "my_set = set(codes_818)\n",
    "\n",
    "print(\"My set length - \"+str(len(my_set)))\n",
    "print(\"Girish set length - \"+str(len(girish_set)))\n",
    "\n",
    "# print(\"Intersection length - \"+str(len(intersection_set)))\n",
    "# intersection_set = my_set.intersection(girish_set)\n",
    "# intersection_list = list(intersection_set)\n",
    "# intersection_list.sort()\n",
    "# print(intersection_list)\n",
    "\n",
    "union_set = my_set.union(girish_set)\n",
    "print(\"Union length - \"+str(len(union_set)))\n",
    "union_list = list(union_set)\n",
    "union_list.sort()\n",
    "print(union_list)\n",
    "\n",
    "\n",
    "with open(\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1330-design-codes/1330-design-codes.txt\", \"w\") as file:\n",
    "    for line in union_list:\n",
    "        file.write(\"%s\\n\"%line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/serialextractor-no-prefix/designCodes-SNs.txt\", \"r\") as file:\n",
    "    design_codes_sns = file.read().splitlines()\n",
    "\n",
    "design_codes_sns_new = []\n",
    "serial_numbers = []\n",
    "for line in design_codes_sns:\n",
    "    code = line.split(\"-\")[0]\n",
    "    if code in design_codes_with_min_support:\n",
    "        design_codes_sns_new.append(line)\n",
    "        serials = line.split(\"-\")[1].split(\",\")\n",
    "        serial_numbers.extend(serials)\n",
    "        \n",
    "with open(\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/818-design-codes/designCodes-SNs.txt\", \"w\") as file:\n",
    "    for line in design_codes_sns_new:\n",
    "        file.write(\"%s\\n\"%line)\n",
    "    \n",
    "with open(\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/818-design-codes/serialNumbers-Flat.txt\", \"w\") as file:\n",
    "    for line in serial_numbers:\n",
    "        file.write(\"%s\\n\"%line)    \n",
    "        \n",
    "serial_numbers_set = set(serial_numbers)        \n",
    "with open(\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/818-design-codes/serialNumbers-Flat-Uniq.txt\", \"w\") as file:\n",
    "    for line in serial_numbers_set:\n",
    "        file.write(\"%s\\n\"%line)            \n",
    "\n",
    "with open(\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/serialextractor-no-prefix/designCode-distribution.txt\", \"r\") as file:\n",
    "    designCode_distribution = file.read().splitlines()\n",
    "                \n",
    "        \n",
    "\n",
    "# 1023539 -flat\n",
    "# 500857 - uniq\n",
    "# 546424 - \n",
    "# 1134957\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "designCode_distribution_new = []\n",
    "for line in designCode_distribution:\n",
    "    code = line.split(\" - \")[0]\n",
    "    if code in design_codes_with_min_support:\n",
    "        designCode_distribution_new.append(line)\n",
    "\n",
    "print(\"New distribution lenght - \"+str(len(designCode_distribution_new)))        \n",
    "with open(\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/818-design-codes/designCode-distribution.txt\", \"w\") as file:\n",
    "    for line in designCode_distribution_new:\n",
    "        file.write(\"%s\\n\"%line)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"abc\"\n",
    "a = a + \"def\"\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1=\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1330-design-codes/test-data/girish_training_serials.txt\"\n",
    "file2=\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1330-design-codes/test-data/serial_numbers_filtered.txt\"\n",
    "\n",
    "with open(file1, \"r\") as file:    \n",
    "    data1=set(file.read().splitlines())\n",
    "with open(file2, \"r\") as file:\n",
    "    data2=set(file.read().splitlines())\n",
    "    \n",
    "data3 = data2.intersection(data1)    \n",
    "\n",
    "# data4 = []\n",
    "# for serial in data2:\n",
    "#     if serial not in data3:\n",
    "#         data4.append(serial)\n",
    "        \n",
    "# print(\"{} - {}\".format(len(data2), len(data4)))\n",
    "\n",
    "# with open(\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1330-design-codes/test-data/serial_numbers_filtered.txt\", \"w\") as file:\n",
    "#     for line in data4:\n",
    "#         file.write(\"%s\\n\"%line)            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(len(data1)))\n",
    "print(str(len(data2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data2.intersection(data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = \"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/test-data-1/design_to_serial_output.txt\"\n",
    "with open(file1, \"r\") as file:    \n",
    "    data1=file.read().splitlines()\n",
    "\n",
    "empty_lines = [line for line in data1 if len(line.split(\"-\")[1]) == 0]\n",
    "print(len(empty_lines))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1=\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1330-design-codes/test-data/girish_training_serials.txt\"\n",
    "file2=\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/818-design-codes/serialNumbers-Flat-Uniq.txt\"\n",
    "testing_file=\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/serials_refiles_all.txt\"\n",
    "\n",
    "with open(file1, \"r\") as file:    \n",
    "    data1=set(file.read().splitlines())\n",
    "with open(file2, \"r\") as file:\n",
    "    data2=set(file.read().splitlines())\n",
    "with open(testing_file, \"r\") as file:\n",
    "    testing_data=set(file.read().splitlines())\n",
    "\n",
    "print(len(testing_data))    \n",
    "for item in data1:\n",
    "    if item in testing_data:\n",
    "        testing_data.remove(item)\n",
    "\n",
    "print(len(testing_data))    \n",
    "for item in data2:\n",
    "    if item in testing_data:\n",
    "        testing_data.remove(item)\n",
    "\n",
    "print(len(testing_data))    \n",
    "\n",
    "with open(\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1330-design-codes/test-data-2/serial_numbers.txt\", \"w\") as file:\n",
    "    for line in testing_data:\n",
    "        file.write(\"%s\\n\"%line)            \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1=\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/mark_desc_output_new.csv\"\n",
    "serials1 = []\n",
    "with open(file1, \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:        \n",
    "        serials1.append(row[1])\n",
    "\n",
    "file2=\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/test-data-1/serials_output.txt\"\n",
    "with open(file2, \"r\") as file:\n",
    "    serials2=file.read().splitlines()\n",
    "print(len(serials2))\n",
    "non_matching_indices = []\n",
    "for index,serial in enumerate(serials2):\n",
    "    if not serial in serials1:\n",
    "        print(str(serial)+\" - \"+str(index))\n",
    "        \n",
    "print(non_matching_indices)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1=\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/mark_desc_output.csv\"\n",
    "serials1 = []\n",
    "with open(file1, \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:        \n",
    "        serials1.append(row[1])\n",
    "lines = []\n",
    "with open(file1, \"r\") as file:\n",
    "    lines=file.read().splitlines()\n",
    "        \n",
    "file2=\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/test-data-1/serials_output.txt\"\n",
    "with open(file2, \"r\") as file:\n",
    "    serials2=file.read().splitlines()\n",
    "    \n",
    "lines_new=[]\n",
    "for index,serial in enumerate(serials2):\n",
    "    if not serial in serials1:\n",
    "        print(str(serial)+\" - \"+str(index))\n",
    "        lines_new.append(serial)\n",
    "    else:        \n",
    "        lines_new.append(lines[serials1.index(serial)])\n",
    "        \n",
    "file4=\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/mark_desc_output_new.csv\"\n",
    "with open(file4, \"w\") as file:\n",
    "    for line in lines_new:\n",
    "        file.write(\"%s\\n\"%line)            \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to generate ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_results_cr_file = \"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/test-data-1/text-results-1/sashi_data_classification_report.txt\"\n",
    "image_results_cr_file = \"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/test-data-1/image-results-1/classification_report.txt\"\n",
    "ensemble_cr_file = \"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/test-data-1/classification_report_ensemble.txt\"\n",
    "\n",
    "text_cr = pd.read_csv(text_results_cr_file,delimiter=\"\\s+\", index_col=0, skipfooter=2)\n",
    "images_cr = pd.read_csv(image_results_cr_file, delimiter=\"\\s+\", index_col=0, skipfooter=2)\n",
    "ensemble_cr = pd.read_csv(ensemble_cr_file, delimiter=\"\\s+\", index_col=0, skipfooter=2)\n",
    "\n",
    "df = pd.DataFrame(columns=['precision', 'recall', 'f1-score', 'image-f1-socre', 'text-f1-score', 'support'])\n",
    "\n",
    "design_codes_file = \"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/1381-design-codes.txt\"\n",
    "with open(design_codes_file, \"r\") as file:\n",
    "    design_codes = file.read().splitlines()\n",
    "\n",
    "for code in design_codes:\n",
    "    code_no_period = int(code.replace(\".\", \"\"))\n",
    "    image_f1_score = float('nan')\n",
    "    try:\n",
    "        image_f1_score = images_cr.loc[code][\"f1-score\"]\n",
    "    except KeyError:            \n",
    "        image_f1_score = float('nan')\n",
    "    text_cr.loc[code_no_period][\"f1-score\"]\n",
    "    df.loc[code] = [ensemble_cr.loc[code][\"precision\"], \n",
    "                    ensemble_cr.loc[code][\"recall\"], \n",
    "                    ensemble_cr.loc[code][\"f1-score\"], \n",
    "                    image_f1_score,\n",
    "                    text_cr.loc[code_no_period][\"f1-score\"],\n",
    "                    ensemble_cr.loc[code][\"support\"] ]\n",
    "\n",
    "    \n",
    "ensemble_cr_file_composite = \"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/test-data-1/classification_report_ensemble_composite.txt\"    \n",
    "# df.to_csv(ensemble_cr_file_composite, sep=\" \")\n",
    "print(df.to_string())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns=['design_code', 'precision'], )\n",
    "df.loc[\"01.01.01\"] = [\"01.01.02\", 1.2]\n",
    "\n",
    "print(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_results_cr_file = \"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/test-data-1/image-results-1/classification_report.txt\"\n",
    "\n",
    "images_cr = pd.read_csv(image_results_cr_file, delimiter=\"\\s+\", index_col=0, skipfooter=2)\n",
    "\n",
    "lessthan10=0\n",
    "lessthan20=0\n",
    "lessthan30=0\n",
    "lessthan40=0\n",
    "greaterthan40 = 0\n",
    "for i in range(len(images_cr)):\n",
    "    score = images_cr.iloc[i][\"f1-score\"]\n",
    "    if score <= 0.10:\n",
    "        lessthan10 = lessthan10+1\n",
    "    elif score <= 0.20:\n",
    "        lessthan20 = lessthan20 + 1\n",
    "    elif score <= 0.30:\n",
    "        lessthan30 = lessthan30 + 1\n",
    "    elif score <= 0.40:\n",
    "        lessthan40 = lessthan40 + 1\n",
    "    else:\n",
    "        greaterthan40 = greaterthan40 + 1\n",
    "\n",
    "print(\"{} {} {} {} {}\".format(lessthan10, lessthan20, lessthan30, lessthan40, greaterthan40))\n",
    "new_codes = []\n",
    "for i in range(len(images_cr)):\n",
    "    score = images_cr.iloc[i][\"f1-score\"]\n",
    "    if(score > 0.1):\n",
    "        new_codes.append(images_cr.index.values[i])\n",
    "\n",
    "\n",
    "with open(\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/test-data-1/image-results-1/bestPerformingDesignCodes/best_performing_codes.txt\", \"w\") as file:\n",
    "    for line in new_codes:\n",
    "        file.write(\"%s\\n\"%line)  \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We have results from image training of 818 design codes. \n",
    "we have identified 640 design codes that have better scores. \n",
    "So, we are trying to see how well it did if we only consider the 640 design codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/test-data-1/image-results-1/bestPerformingDesignCodes\"\n",
    "actual_results_file=\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/test-data-1/serial_to_design_output.txt\"\n",
    "predictions_file=\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/test-data-1/image-results-1/weighted_predictions.pickle\"\n",
    "subset_design_codes_file = base_folder+\"/640-design-codes.txt\"\n",
    "superset_design_codes_file = \"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/test-data-1/818-design-codes.txt\"\n",
    "testing_input_serials_file = \"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/test-data-1/serials_output.txt\"\n",
    "predictions_json_file = \"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/test-data-1/image-results-1/predictions.txt\"\n",
    "\n",
    "with open(predictions_json_file, \"r\") as file:\n",
    "    predicted_serials = list(json.load(file)[\"labels\"].keys())\n",
    "    predicted_serials.sort()\n",
    "\n",
    "with open(testing_input_serials_file, \"r\") as file:\n",
    "    testing_input_serials = file.read().splitlines()\n",
    "\n",
    "with open(subset_design_codes_file, \"r\") as file:\n",
    "    subset_design_codes = file.read().splitlines()\n",
    "    \n",
    "with open(superset_design_codes_file, \"r\") as file:\n",
    "    superset_design_codes = file.read().splitlines()\n",
    "\n",
    "serial_numbers_temp = []\n",
    "y_actuals_temp = []\n",
    "with open(actual_results_file, \"r\") as file:\n",
    "    lines = file.read().splitlines()\n",
    "    for line in lines:\n",
    "        serial_number = line.split(\"-\")[0]\n",
    "        label_values = line.split(\"-\")[1].split(\",\")\n",
    "        filtered_label_values = [item for item in label_values if item in subset_design_codes]\n",
    "        if(len(filtered_label_values)) > 0:\n",
    "            serial_numbers_temp.append(serial_number)\n",
    "            y_actuals_temp.append(tuple(filtered_label_values))\n",
    "print(\"Serial numbers temp - \"+str(len(serial_numbers_temp)))       \n",
    "\n",
    "serial_numbers = []\n",
    "y_actuals = []\n",
    "for input_serial in testing_input_serials:\n",
    "    if input_serial in serial_numbers_temp:\n",
    "        index = serial_numbers_temp.index(input_serial)\n",
    "        serial_numbers.append(serial_numbers_temp[index])\n",
    "        y_actuals.append(y_actuals_temp[index])\n",
    "\n",
    "lb = MultiLabelBinarizer(classes=subset_design_codes)\n",
    "lb =  lb.fit(y_actuals)\n",
    "y_actuals_bl = lb.transform(y_actuals)\n",
    "print(\"Shape of y_actuals - \"+str(y_actuals_bl.shape))\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(predictions_file, \"rb\") as file:\n",
    "        predictions = pickle.load(file)\n",
    "\n",
    "print(\"shape of existing predictions - \"+str(predictions.shape))\n",
    "new_predictions = np.zeros(shape=(predictions.shape[0], len(subset_design_codes)), dtype=np.float64)\n",
    "\n",
    "for index, design_code in enumerate(subset_design_codes):\n",
    "    design_code_index = superset_design_codes.index(design_code)\n",
    "    new_predictions[:,index] = predictions[:,design_code_index]\n",
    "\n",
    "new_predictions_2 = np.zeros(shape=(len(serial_numbers), len(subset_design_codes)), dtype=np.float64)\n",
    "\n",
    "for index1, serial in enumerate(serial_numbers):\n",
    "    if serial in predicted_serials:\n",
    "        index2 = predicted_serials.index(serial)\n",
    "        new_predictions_2[index1, :] = new_predictions[index2]\n",
    "\n",
    "print (\"Shape of new predictions - \"+str(new_predictions_2.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = new_predictions_2 >= 0.2\n",
    "report = metrics.classification_report(y_actuals_bl, y_preds, target_names=subset_design_codes)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating network weights for 818 image classification models \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "global model_type\n",
    "network_weights_arr = []\n",
    "model_type = \"finetune\"\n",
    "number_of_networks = 3\n",
    "base_path = \"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/818-design-codes/all_network_crs/all_crs/\"\n",
    "for network_number in range(number_of_networks):    \n",
    "    cr_file = \"{}/classification_report_finetune{}.txt\".format(base_path, str(network_number+1))\n",
    "    cr = pd.read_csv(cr_file, delimiter=\"\\s+\", index_col=0, skipfooter=2)\n",
    "    network_weights_arr.append(cr.loc[:][\"f1-score\"].T)\n",
    "\n",
    "network_weights = np.array(network_weights_arr)\n",
    "labels_size = network_weights.shape[1]\n",
    "sum_weights = np.zeros(shape=(1, labels_size), dtype=np.float32)\n",
    "for index in range(number_of_networks):\n",
    "    sum_weights = sum_weights + network_weights[index]\n",
    "# if the scores sum to zero, we want to avoid a Nan during division\n",
    "sum_weights = np.where(sum_weights == 0, 1, sum_weights)\n",
    "for index in range(number_of_networks):\n",
    "    network_weights[index] = network_weights[index]/sum_weights\n",
    "\n",
    "weights_file = base_path+\"network_weights.pickle\"\n",
    "with open(weights_file, \"wb\") as file:\n",
    "    pickle.dump(network_weights, file)    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating text_weights and image_weights for ensemble model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_codes_superset_file=\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/test-data-1/1381-design-codes.txt\"\n",
    "text_results_cr_file = \"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/test-data-1/text-results-1/classification_report_for_weights.txt\"\n",
    "image_results_cr_file = \"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/test-data-1/image-results-1/classification_report_for_weights.txt\"\n",
    "\n",
    "with open(design_codes_superset_file, \"r\") as file:\n",
    "    design_codes_superset = file.read().splitlines()\n",
    "\n",
    "superset_length = len(design_codes_superset)\n",
    "text_cr = pd.read_csv(text_results_cr_file,delimiter=\"\\s+\", index_col=0, skipfooter=2)\n",
    "images_cr = pd.read_csv(image_results_cr_file, delimiter=\"\\s+\", index_col=0, skipfooter=2)\n",
    "text_weights = np.zeros((1,superset_length), dtype=np.float32)\n",
    "image_weights = np.zeros((1,superset_length), dtype=np.float32)\n",
    "for index, design_code in enumerate(design_codes_superset):\n",
    "    design_code_no_period = design_code.replace(\".\", \"\")\n",
    "    text_f1 = 0\n",
    "    images_f1 = 0\n",
    "    try:\n",
    "        text_f1 = text_cr.loc[int(design_code_no_period)][\"f1-score\"]\n",
    "    except KeyError:\n",
    "        text_f1 = 0\n",
    "    try:\n",
    "        images_f1 = images_cr.loc[design_code][\"f1-score\"]\n",
    "    except KeyError:\n",
    "        images_f1 = 0\n",
    "    sum_f1 = text_f1 + images_f1\n",
    "    if sum_f1 == 0:\n",
    "        sum_f1 = 1\n",
    "    text_weights[0,index] = (text_f1/sum_f1)\n",
    "    image_weights[0,index] = images_f1/sum_f1\n",
    "\n",
    "text_weights_file=\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/test-data-1/ensemble_weights/text_model_weights.pickle\"\n",
    "image_weights_file=\"/Users/greensod/usptoWork/TrademarkRefiles/data/keras/1381-design-codes/test-data-1/ensemble_weights/image_model_weights.pickle\"\n",
    "\n",
    "with open(text_weights_file, \"wb\") as file:\n",
    "    pickle.dump(text_weights, file)    \n",
    "with open(image_weights_file, \"wb\") as file:\n",
    "    pickle.dump(image_weights, file)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,1], [1,1]])\n",
    "b = np.array([[1,1], [1,1]])\n",
    "a*b\n",
    "np.dot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(actuals, y_pred):\n",
    "    actuals = tf.cast(actuals, tf.int32)\n",
    "    threshold = tf.fill(tf.shape(actuals), value=0.3)\n",
    "    predictions = tf.where(y_pred >= threshold, tf.fill(tf.shape(actuals), value=1),\n",
    "                           tf.fill(tf.shape(actuals), value=0))\n",
    "    true_positives = tf.reduce_sum(tf.multiply(actuals, predictions))\n",
    "    actual_positives = tf.reduce_sum(actuals)\n",
    "    recall = tf.divide(true_positives, actual_positives)\n",
    "    return recall\n",
    "\n",
    "def precision(actuals, y_pred):\n",
    "    actuals = tf.cast(actuals, tf.int32)\n",
    "    threshold = tf.fill(tf.shape(actuals), value=0.3)\n",
    "    predictions = tf.where(y_pred >= threshold, tf.fill(tf.shape(actuals), value=1),\n",
    "                           tf.fill(tf.shape(actuals), value=0))\n",
    "    true_positives = tf.reduce_sum(tf.multiply(actuals, predictions))\n",
    "    predicted_positives = tf.reduce_sum(predictions)\n",
    "    precision = tf.divide(true_positives, predicted_positives)\n",
    "    return precision\n",
    "\n",
    "def f1score(actuals, y_pred):\n",
    "    recall_value = recall(actuals, y_pred)\n",
    "    precision_value = precision(actuals, y_pred)\n",
    "    f1_score = ((precision_value * recall_value) / (precision_value + recall_value)) * 2\n",
    "    return f1_score\n",
    "\n",
    "\n",
    "base_folder = \"/data1/ml/818-design-codes/rest_service_testing/\"\n",
    "base_image_resources = base_folder+\"/image_resources\"\n",
    "base_text_config = base_folder+\"/text_resources\"\n",
    "all_design_codes_file = base_folder+\"/1381-design-codes.txt\"\n",
    "image_design_codes_file = base_folder+\"/818-design-codes.txt\"\n",
    "text_design_codes_file = base_folder+\"/1381-design-codes.txt\"\n",
    "image_network_weights_file=base_image_resources+\"/image_network_weights.pickle\"\n",
    "number_of_image_networks = 3\n",
    "\n",
    "with open(all_design_codes_file, 'r') as file:\n",
    "    all_design_codes = file.read().splitlines()\n",
    "with open(image_design_codes_file, 'r') as file:\n",
    "    image_design_codes = file.read().splitlines()\n",
    "with open(text_design_codes_file, 'r') as file:\n",
    "    text_design_codes = file.read().splitlines()\n",
    "with open(image_network_weights_file, \"rb\") as file:\n",
    "    image_network_weights = pickle.load(file)\n",
    "\n",
    "\n",
    "class ImagePredictor:\n",
    "    log = logging.getLogger(__qualname__)\n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "        self.all_design_codes_size = len(all_design_codes)\n",
    "        self.image_design_codes_size = len(image_design_codes)\n",
    "        for i in range(number_of_image_networks):\n",
    "#             model_file = \"{}/model{}.h5\".format(base_image_resources,str(i+1))\n",
    "            model_file = \"/data1/ml/818-design-codes/group1/network{}/model_finetune1.h5\".format(str(i))\n",
    "            self.models.append(keras.models.load_model(model_file,custom_objects={\"recall\": recall, \"precision\": precision, \"f1score\": f1score}))\n",
    "        self.image_adjuster_multiplier = np.zeros((self.image_design_codes_size, self.all_design_codes_size))\n",
    "        for index1, design_code in enumerate(image_design_codes):\n",
    "            index2 = all_design_codes.index(design_code)\n",
    "            self.image_adjuster_multiplier[index1, index2] = 1\n",
    "\n",
    "    def get_image_predictions(self, image_arr):\n",
    "        predictions_array = []\n",
    "        for model in self.models:\n",
    "            predictions = model.predict(image_arr, verbose=1)[0]\n",
    "            predictions_array.append(predictions)\n",
    "#             print(\"model predictions - \"+str(np.nonzero(predictions)[0]))\n",
    "\n",
    "        weighted_predictions = np.zeros(shape=(predictions_array[0].shape))\n",
    "        for index, predictions in enumerate(predictions_array):\n",
    "            weighted_predictions = weighted_predictions  + (predictions*image_network_weights[index])\n",
    "\n",
    "        predictions = np.dot(weighted_predictions, self.image_adjuster_multiplier)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "predictor = ImagePredictor()    \n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file = h5py.File(\"/data1/ml/818-design-codes/all_groups/data-299.h5\", mode='r')\n",
    "print(\"shape of image - \"+str(hdf5_file[\"images_testing\"][0].shape))\n",
    "\n",
    "\n",
    "# with open(\"/Users/greensod/usptoWork/TrademarkRefiles/data/tsdrImages/Jan-2011/85208988.jpg\", \"rb\") as file:\n",
    "    # image_buffer = bytearray(file.read())\n",
    "    # image_arr = Utils.getImageVector(image_buffer)\n",
    "    # predictions = ImagePredictor().get_image_predictions(image_arr)\n",
    "# img = cv2.imread(\"/data1/ml/818-design-codes/group1/all/01.01.10/86709607.jpg\")\n",
    "# img = cv2.imread(\"/data1/ml/818-design-codes/all_groups/testing/76720232.jpg\")\n",
    "img = cv2.imread(\"/data1/ml/818-design-codes/rest_service_testing/images/76720232.png\")\n",
    "\n",
    "img = cv2.resize(img, (299, 299), interpolation=cv2.INTER_CUBIC)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = hdf5_file[\"images_testing\"][0]\n",
    "print(img.shape)\n",
    "plt.imshow(img, interpolation='nearest')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "img = img/255\n",
    "predictions = predictor.get_image_predictions(img[None])\n",
    "predictions = predictions > 0.2\n",
    "print (str(predictions))\n",
    "print(\"Shape of preds - \"+ str(np.nonzero(predictions)))\n",
    "indices = np.nonzero(predictions)[0]\n",
    "for i in indices:\n",
    "    print(str(all_design_codes[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the text classification predictions piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.applications import *\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import backend as k\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras import applications\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import h5py\n",
    "# import matplotlib as mpl\n",
    "# mpl.use('TkAgg')\n",
    "# import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pickle\n",
    "import cv2\n",
    "from timeit import default_timer as timer\n",
    "from urllib import request\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder =\"/data1/ml/818-design-codes/rest_service_testing/\";\n",
    "base_image_resources = base_folder+\"/image_resources\"\n",
    "base_text_config = base_folder+\"/text_resources\"\n",
    "all_design_codes_file = base_folder+\"/1381-design-codes.txt\"\n",
    "image_design_codes_file = base_folder+\"/818-design-codes.txt\"\n",
    "text_design_codes_file = base_folder+\"/1381-design-codes.txt\"\n",
    "\n",
    "image_network_weights_file=base_image_resources+\"/image_network_weights.pickle\"\n",
    "text_ensemble_weights=base_folder+\"/text_model_weights.pickle\"\n",
    "image_ensemble_weights=base_folder+\"/image_ensemble_weights.pickle\"\n",
    "\n",
    "text_stop_words_file = base_text_config+\"/customstopwords.csv\"\n",
    "text_tokenizer_input_file = base_text_config+\"/tokenizer_input.txt\"\n",
    "text_model_weights= base_text_config+\"/200seq_300d_1381_BI_GRU_tm_embedding.hdf5\"\n",
    "text_embeddings_file = base_text_config+\"/latest_tm_markdesc_300.vec\"\n",
    "\n",
    "\n",
    "number_of_image_networks = 1\n",
    "\n",
    "with open(all_design_codes_file, 'r') as file:\n",
    "    all_design_codes = file.read().splitlines()\n",
    "with open(image_design_codes_file, 'r') as file:\n",
    "    image_design_codes = file.read().splitlines()\n",
    "with open(text_design_codes_file, 'r') as file:\n",
    "    text_design_codes = file.read().splitlines()\n",
    "with open(image_network_weights_file, \"rb\") as file:\n",
    "    image_network_weights = pickle.load(file)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "MAX_NB_WORDS = 50000\n",
    "EMBEDDING_DIM = 300\n",
    "PREDICTIONS_THRESHOLD=0.3\n",
    "\n",
    "\n",
    "with open(text_tokenizer_input_file, \"r\") as file:\n",
    "    tokenizer_input_list = file.read().splitlines()\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n",
    "tokenizer.fit_on_texts(tokenizer_input_list)\n",
    "print(\"Tokenizer fit done\")\n",
    "embeddings_index = {}\n",
    "f = open(text_embeddings_file)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "print(\"Created embedding layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading model.')\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "units = 128\n",
    "conv_filters = 32\n",
    "x = Dropout(0.2)(embedded_sequences)\n",
    "x = Bidirectional(GRU(\n",
    "    units,\n",
    "    dropout=0.2,\n",
    "    recurrent_dropout=0.2,\n",
    "    return_sequences=True))(x)\n",
    "x = Reshape((2 * MAX_SEQUENCE_LENGTH, units, 1))(x)\n",
    "x = Conv2D(conv_filters, (3, 3))(x)\n",
    "\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "preds = Dense(len(text_design_codes), activation='sigmoid')(x)  # for multilabel classification # length of labels\n",
    "model = Model(sequence_input, preds)\n",
    "model.load_weights(text_model_weights)\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_stop_words_file, \"r\") as file:\n",
    "    custom_words = file.read().splitlines()\n",
    "en_stop = set(stopwords.words('english'))\n",
    "en_stop.update(\n",
    "    ['.', '(', '(', ')', 'u', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}', '&', '/', '-', '+',\n",
    "     '*', \\\n",
    "     \"..\", '...', '....', \"+...\", \"-...\", \",...\", \"'...\", \"!!!\", \"&...\", \"(...\", \")...\", \"]...\", \"/...\", \"(+)...\",\n",
    "     \"),...\", \\\n",
    "     \"),\", \").\", \"):\", \")-\", \"))\", \"])\", \".)\", \"!)\", \"')\", \"][\", '\").', '\")/', '\",', '\"-', '\")', '\"/', './', '--', \\\n",
    "     \"#:\", \"(+)\", \"($\", \"-$\", \"/\", \"+/\", \",\", \"+-\", \"(#\", \"''\"])\n",
    "stop_words = en_stop.union(custom_words)\n",
    "\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    return ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "\n",
    "mark_desc = 'stylized lion circle'\n",
    "\n",
    "input_str = remove_stop_words(mark_desc)\n",
    "input_str = re.sub('([^\\s\\w]|_)+', ' ', input_str).strip()\n",
    "input_str = re.sub('\\s+', ' ', input_str)  # remove multiple spaces\n",
    "input_str = [str(input_str.lower())]\n",
    "print(\"input str - \"+str(input_str))\n",
    "input = tokenizer.texts_to_sequences(input_str)\n",
    "input = sequence.pad_sequences(input, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print(input)\n",
    "raw_predictions = model.predict(input, verbose=0)[0]\n",
    "print(raw_predictions)\n",
    "label_indices = np.nonzero(raw_predictions > 0.3)[0]\n",
    "print(label_indices)\n",
    "labels = []\n",
    "for i in label_indices:\n",
    "    labels.append(all_design_codes[i])\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
