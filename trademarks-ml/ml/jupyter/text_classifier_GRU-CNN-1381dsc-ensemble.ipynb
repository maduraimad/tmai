{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload all modules (except those excluded by %aimport) every time before executing the Python code typed.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import absolute_import\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "import pickle\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "import string #print (string.punctuation)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Classifiers\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "#helpers\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging,re\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "#initializersfrom keras import initializers\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM,Bidirectional,TimeDistributed,InputLayer,GRU\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Conv2D, MaxPool2D, Reshape, Flatten\n",
    "from keras.layers import SpatialDropout1D, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Merge\n",
    "from keras.layers.convolutional import Convolution2D,MaxPooling2D,Convolution1D,MaxPooling1D,UpSampling2D, ZeroPadding2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install tensorflow\n",
    "#!{sys.executable} -m pip install keras\n",
    "#!{sys.executable} -m pip install pydot\n",
    "#!{sys.executable} -m pip install graphviz\n",
    "#!{sys.executable} -m pip install mlxtend  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflow.data import get_trademark_data\n",
    "from workflow.data import prepare_Xy\n",
    "\n",
    "from workflow.utilities import save_object\n",
    "from workflow.utilities import load_object\n",
    "\n",
    "from workflow.fasttextwrap import fasttextClassifier\n",
    "\n",
    "from classifiers.kerasTextClassifier import create_bi_lstm_model\n",
    "from classifiers.kerasTextClassifier import create_model\n",
    "from workflow.train import getStopWords\n",
    "from workflow.train import trainFastTextModel\n",
    "from workflow.train import cleanData\n",
    "from workflow.train import clean_text\n",
    "\n",
    "from workflow.test import test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "##########################################\n",
    "TODO\n",
    "-add ability for stratified sampling\n",
    "##########################################\n",
    "'''\n",
    "random_state = 47\n",
    "test_size = .99\n",
    "stopwords_file = 'customstopwords.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordsList = getStopWords()\n",
    "en_stop = set(stopwords.words('english'))\n",
    "en_stop.update(['.','(','(', ')','u',',', '\"', \"'\", '?', '!', ':', ';' ,'(', ')', '[', ']', '{', '}', '&', '/', '-', '+', '*',\\\n",
    "                       \"..\", '...', '....', \"+...\", \"-...\", \",...\", \"'...\", \"!!!\", \"&...\", \"(...\", \")...\", \"]...\", \"/...\", \"(+)...\", \"),...\",\\\n",
    "                       \"),\", \").\", \"):\", \")-\", \"))\", \"])\", \".)\", \"!)\", \"')\", \"][\", '\").', '\")/', '\",', '\"-', '\")', '\"/', './', '--',\\\n",
    "                       \"#:\", \"(+)\", \"($\", \"-$\", \"/\",\"+/\",\",\", \"+-\", \"(#\", \"''\"])\n",
    "stopWords = en_stop.union(stopwordsList)\n",
    "#print (stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "##########################################\n",
    "TODO\n",
    "-Add ability to select subset of labels\n",
    "-Add ability to set data_dir (various impacts throughout files)\n",
    "##########################################\n",
    "'''\n",
    "data = get_trademark_data(\n",
    "    statement_url='https://bulkdata.uspto.gov/data/trademark/casefile/economics/2016/statement.csv.zip',\n",
    "    design_url='https://bulkdata.uspto.gov/data/trademark/casefile/economics/2016/design_search.csv.zip',\n",
    "    sample_size=1, #1 means 100% of the data. Lower it (.1) to run faster when testing script\n",
    "    test_size=0.99, #Percentage of data to set aside for testing\n",
    "    filename='trademark_data_updated_Naresh.csv', #save retrieved data as this name, if exists will load .csv instead of url download\n",
    "    data_dir='', #save retrieved data in this folder\n",
    "    force_download=False #redownload from url and overwrite and saved .csv\n",
    "    )\n",
    "\n",
    "data.head()\n",
    "\n",
    "X,y = prepare_Xy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "##########################################\n",
    "TODO\n",
    "-add analysis jupyter workbook\n",
    "\n",
    "##########################################\n",
    "'''\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(y)\n",
    "print (len (mlb.classes_))\n",
    "#print (mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for value in mlb.classes_:\n",
    "    #print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "copyofX = copy.deepcopy(X)\n",
    "#copyofX\n",
    "len(copyofX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __remove_stop_words(text):\n",
    "    return ' '.join([word for word in text.split() if word.lower() not in stopWords])\n",
    "\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,3}\\b')\n",
    "myList= []\n",
    "for sentence in copyofX :\n",
    "    #print (sentence)\n",
    "    sentence= shortword.sub('', sentence)\n",
    "    sentence = __remove_stop_words(sentence)\n",
    "    sentence=re.sub('([^\\s\\w]|_)+',' ',sentence).strip() \n",
    "    #sentence=re.sub('[?!\\\\,]+','',sentence).strip() #remove comma from sentence\n",
    "    #sentence=re.sub('[?!\\\\.]+','',sentence).strip() #remove punctuation from sentence because we don't need it anymore\n",
    "    #sentence=re.sub('[?!\\\\\"]+','',sentence).strip() #remove quotes from sentence\n",
    "    #sentence=re.sub('[?!\\\\-]+','',sentence).strip() #remove dash from sentence\n",
    "    sentence=re.sub('\\s+',' ',sentence)\t# remove multiple spaces\n",
    "    myList.append(sentence.lower())\n",
    "    #print (sentence.lower())\n",
    "print (len(myList))\n",
    "print (myList[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "##########################################\n",
    "TODO\n",
    "-add ability to have multiple model types\n",
    "-seperate out preprocessing and vect from fast text\n",
    "-add other models\n",
    "-select variables\n",
    "\n",
    "##########################################\n",
    "'''\n",
    "\n",
    "# import importlib\n",
    "# importlib.reload(workflow.fasttextwrap)\n",
    "# from workflow.fasttextwrap import fasttextClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(myList, y, test_size=test_size, random_state=random_state)\n",
    "print(X_train[0:2])\n",
    "print(X_test[0:2])\n",
    "#print (y_train[0:2])\n",
    "#print (y_test[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 200\n",
    "MAX_NB_WORDS = 50000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 1\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n",
    "tokenizer.fit_on_texts(myList)\n",
    "sequences = tokenizer.texts_to_sequences(myList)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "#labels = to_categorical(np.asarray(labels)) #aaron removed\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "y = y[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = y[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = y[-num_validation_samples:]\n",
    "\n",
    "print('shape of:', x_train.shape)\n",
    "print('shape of:',y_train.shape)\n",
    "print('shape of:',x_val.shape)\n",
    "print('shape of:',y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.layers import Embedding\n",
    "\n",
    "BASE_DIR = ''\n",
    "#GLOVE_DIR = os.path.join(BASE_DIR, 'glove_embedding')\n",
    "#TEXT_DATA_DIR = os.path.join(BASE_DIR, 'glove.6B.300d.txt')\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'fasttext_word_representations')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, 'latest_tm_markdesc_300.vec')\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "MAX_NB_WORDS = 50000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'latest_tm_markdesc_300.vec'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                           EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflow.data import get_trademark_data\n",
    "from workflow.data import prepare_Xy\n",
    "\n",
    "pred_data = get_trademark_data(\n",
    "    statement_url='https://bulkdata.uspto.gov/data/trademark/casefile/economics/2016/statement.csv.zip',\n",
    "    design_url='https://bulkdata.uspto.gov/data/trademark/casefile/economics/2016/design_search.csv.zip',\n",
    "    sample_size=1, #1 means 100% of the data. Lower it (.1) to run faster when testing script\n",
    "    test_size=.2, #Percentage of data to set aside for testing\n",
    "    filename='sashi_1381_test_mark_desc_output.csv', #save retrieved data as this name, if exists will load .csv instead of url download\n",
    "    data_dir='', #save retrieved data in this folder\n",
    "    force_download=False #redownload from url and overwrite and saved .csv\n",
    "    )\n",
    "\n",
    "pred_data.head()\n",
    "\n",
    "pred_X,pred_y = prepare_Xy(pred_data)\n",
    "print (len (pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "pred_y = mlb.fit_transform(pred_y)\n",
    "#print (mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "pred_copyofX = copy.deepcopy(pred_X)\n",
    "#copyofX\n",
    "len(pred_copyofX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __remove_stop_words(text):\n",
    "    return ' '.join([word for word in text.split() if word.lower() not in stopWords])\n",
    "\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,3}\\b')\n",
    "predList= []\n",
    "for sentence in pred_copyofX :\n",
    "    #print (sentence)\n",
    "    if not (pd.isnull(sentence)):\n",
    "        sentence= shortword.sub('', sentence)\n",
    "        sentence = __remove_stop_words(sentence)\n",
    "        sentence=re.sub('([^\\s\\w]|_)+',' ',sentence).strip() \n",
    "        #sentence=re.sub('[?!\\\\,]+','',sentence).strip() #remove comma from sentence\n",
    "        #sentence=re.sub('[?!\\\\.]+','',sentence).strip() #remove punctuation from sentence because we don't need it anymore\n",
    "        #sentence=re.sub('[?!\\\\\"]+','',sentence).strip() #remove quotes from sentence\n",
    "        #sentence=re.sub('[?!\\\\-]+','',sentence).strip() #remove dash from sentence\n",
    "        sentence=re.sub('\\s+',' ',sentence)\t# remove multiple spaces\n",
    "        predList.append(sentence.lower())\n",
    "        #print (sentence.lower())\n",
    "    else:\n",
    "        #print (sentence)\n",
    "        sentence = \" \"\n",
    "        predList.append(sentence)\n",
    "print (len(predList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigru_cnn():\n",
    "    print('Loading model.')\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    units = 128\n",
    "    conv_filters = 32\n",
    "    x = Dropout(0.2)(embedded_sequences)\n",
    "    x = Bidirectional(GRU(\n",
    "        units,\n",
    "        dropout=0.2,\n",
    "        recurrent_dropout=0.2,\n",
    "        return_sequences=True))(x)\n",
    "    x = Reshape((2 * MAX_SEQUENCE_LENGTH, units, 1))(x)\n",
    "    x = Conv2D(conv_filters, (3, 3))(x)\n",
    "\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    preds = Dense(y_train.shape[1], activation='sigmoid')(x) # for multilabel classification # length of labels\n",
    "    model = Model(sequence_input, preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigru_cnn_model = bigru_cnn()\n",
    "bigru_cnn_model.load_weights('200seq_300d_1381_BI_GRU_tm_embedding.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __remove_stop_words(text):\n",
    "    return ' '.join([word for word in text.split() if word.lower() not in stopWords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "dtype1 = {'serial_no':np.dtype('U'), 'statement_text':np.dtype('U')}\n",
    "data=pd.read_csv(\"sashi_1381_test_mark_desc_output.csv\",dtype=dtype1,usecols=(1,2,3), converters={'design_search_cd':literal_eval})\n",
    "#print (data)\n",
    "\n",
    "#statement_data = pd.read_csv(\"mark_desc_output.csv\")\n",
    "#print (statement_data)\n",
    "#subcolumns = statement_data[['serial_no','statement_text','design_search_cd']]\n",
    "#dsc = list(subcolumns['design_search_cd'])\n",
    "# add the leading 0 to search codes that had it removed\n",
    "#dsc = list (subcolumns['design_search_cd'].astype(str).str.zfill(6))\n",
    "\n",
    "#print (dsc[0:3])\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(list(data['design_search_cd']))\n",
    "#print (y)\n",
    "print (len (mlb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for value in mlb.classes_:\n",
    "    #print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from keras.utils.np_utils import to_categorical\n",
    "    \n",
    "test_txt = ['stylized lion mermaid']\n",
    "#test_txt = ['stylized lion circle', 'mermaid cup vapor']\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "MAX_NB_WORDS = 50000\n",
    "VALIDATION_SPLIT = 1\n",
    "print (len(myList))\n",
    "print (len(predList))\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n",
    "tokenizer.fit_on_texts(myList)\n",
    "sequences = tokenizer.texts_to_sequences(predList)\n",
    "#print (sequences)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "#print (data)\n",
    "#labels = to_categorical(np.asarray(labels)) #aaron removed\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "#np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "y1 = pred_y[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = y1[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = y1[-num_validation_samples:]\n",
    "\n",
    "print('shape of x_train:', x_train.shape)\n",
    "print('shape of y_train:',y_train.shape)\n",
    "print('shape of x_val:',x_val.shape)\n",
    "print('shape of y_val:',y_val.shape)\n",
    "\n",
    "\n",
    "test_pred = bigru_cnn_model.predict(x_val, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (predList[:1])\n",
    "print (test_pred.shape)\n",
    "#print (new_test_pred.shape)\n",
    "print (len(test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print (len(test_pred))\n",
    "save_object(mlb, 'sashi_test_data/sashi_data_mlb.sav')\n",
    "save_object(test_pred, 'sashi_test_data/sashi_data_predictions.sav')\n",
    "with open(\"sashi_test_data/sashi_data_predicted1381designcodes.txt\", \"w\") as new_text_file:\n",
    "    for x1 in range(len(test_pred)):\n",
    "        #print (mlb.classes_[np.nonzero(test_pred[x1])[0]])\n",
    "        x1 = mlb.classes_[np.nonzero(test_pred[x1])[0]]\n",
    "        new_text_file.write(str(x1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_prob=.3\n",
    "with open(\"sashi_test_data/sashi_data_1381_predcodes_greater_0.3.txt\", \"w\") as new_text_file:\n",
    "    new_test_pred = np.where (test_pred > prediction_prob, 1, 0)\n",
    "    for x2 in range(len(new_test_pred)):\n",
    "        x2 = mlb.classes_[np.nonzero(new_test_pred[x2])[0]]\n",
    "        new_text_file.write(str(x2))\n",
    "        new_text_file.write(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val, new_test_pred, target_names=mlb.classes_)) #predict_proba scores by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val, new_test_pred, target_names=mlb.classes_)) #predict_proba scores by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val, new_test_pred, target_names=mlb.classes_)) #predict_proba scores by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __remove_stop_words(text):\n",
    "    return ' '.join([word for word in text.split() if word.lower() not in stopWords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from keras.utils.np_utils import to_categorical\n",
    "    \n",
    "test_txt = ['one star with four points ']\n",
    "#test_txt = ['THE TRADE-MARK IS OF AN OVAL SHAPE, HAVING A MARROON BACKGROUND WITH THE LETTERS BEING SOLID GOLD IN COLOR']\n",
    "#test_txt = ['THE MARK COMPRISES A FANCIFUL ARRANGEMENT OF LETTER COMPRISING THE WORD CHYPRE OVER A DESIGN']\n",
    "test_txt = __remove_stop_words(test_txt[0])\n",
    "test_txt=re.sub('([^\\s\\w]|_)+',' ',test_txt).strip() \n",
    "test_txt=re.sub('\\s+',' ',test_txt)\t# remove multiple spaces\n",
    "test_txt = [str (test_txt.lower())]\n",
    "print (test_txt)\n",
    "#test_txt = ['stylized lion circle', 'mermaid cup vapor']\n",
    "prediction_prob=.3\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "MAX_NB_WORDS = 50000\n",
    "VALIDATION_SPLIT = 1\n",
    "print (len(myList))\n",
    "print (len(test_txt))\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n",
    "tokenizer.fit_on_texts(myList)\n",
    "prediction_sequences = tokenizer.texts_to_sequences(test_txt)\n",
    "#print (sequences)\n",
    "prediction_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(prediction_word_index))\n",
    "\n",
    "prediction_data = pad_sequences(prediction_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print (prediction_data)\n",
    "#labels = to_categorical(np.asarray(labels)) #aaron removed\n",
    "print('Shape of data tensor:', prediction_data.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "indices = np.arange(prediction_data.shape[0])\n",
    "#np.random.shuffle(indices)\n",
    "prediction_data = prediction_data[indices]\n",
    "prediction_y1 = pred_y[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "prediction_x_train = prediction_data[:-num_validation_samples]\n",
    "prediction_y_train = prediction_y1[:-num_validation_samples]\n",
    "prediction_x_val = prediction_data[-num_validation_samples:]\n",
    "prediction_y_val = prediction_y1[-num_validation_samples:]\n",
    "\n",
    "print('shape of x_train:', prediction_x_train.shape)\n",
    "print('shape of y_train:',prediction_y_train.shape)\n",
    "print('shape of x_val:',prediction_x_val.shape)\n",
    "print('shape of y_val:',prediction_y_val.shape)\n",
    "\n",
    "\n",
    "test_prediction = bigru_cnn_model.predict(prediction_x_val, verbose=0)\n",
    "new_test_prediction = np.where (test_prediction > prediction_prob, 1, 0)\n",
    "print (new_test_prediction)\n",
    "new_predictions_list = [x.replace(\" \", \",\") for x in mlb.classes_[np.nonzero(new_test_prediction)[1]]]\n",
    "print (new_predictions_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
