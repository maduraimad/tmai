{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload all modules (except those excluded by %aimport) every time before executing the Python code typed.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"/data/tm_goods_services_classification/tm_goods_services.csv\",sep =',',header=None)\n",
    "#df = pd.read_csv(\"/home/ubuntu/multilabel-text-classification/trademark_data_1003dsc.csv\")\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install numpy\n",
    "#!{sys.executable} -m pip install tensorflow\n",
    "#!{sys.executable} -m pip install keras\n",
    "#!{sys.executable} -m pip install sklearn\n",
    "#!{sys.executable} -m pip install nltk\n",
    "#!{sys.executable} -m pip install pandas\n",
    "#!{sys.executable} -m pip install google\n",
    "#!{sys.executable} -m pip install future\n",
    "#!{sys.executable} -m pip install np_utils\n",
    "#!{sys.executable} -m pip install cython\n",
    "#!{sys.executable} -m pip install fasttext\n",
    "#!{sys.executable} -m pip  install h5py\n",
    "#import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from __future__ import absolute_import\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "import pickle\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "import string #print (string.punctuation)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Classifiers\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "#helpers\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging,re\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "#initializersfrom keras import initializers\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, SeparableConv2D,BatchNormalization\n",
    "from keras.layers import LSTM,Bidirectional,TimeDistributed,InputLayer,GRU\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Conv2D, MaxPool2D, Reshape, Flatten\n",
    "from keras.layers import SpatialDropout1D, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.convolutional import Convolution2D,MaxPooling2D,Convolution1D,MaxPooling1D,UpSampling2D, ZeroPadding2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflow.data import get_trademark_data\n",
    "from workflow.data import prepare_Xy\n",
    "from workflow.data import prepare_Xy_goodsservices_data\n",
    "\n",
    "from workflow.utilities import save_object\n",
    "from workflow.utilities import load_object\n",
    "\n",
    "#from workflow.fasttextwrap import fasttextClassifier\n",
    "\n",
    "#from classifiers.kerasTextClassifier import create_bi_lstm_model\n",
    "#from classifiers.kerasTextClassifier import create_model\n",
    "#from workflow.train import getStopWords\n",
    "#from workflow.train import trainFastTextModel\n",
    "#from workflow.train import cleanData\n",
    "#from workflow.train import clean_text\n",
    "\n",
    "#from workflow.test import test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "##########################################\n",
    "TODO\n",
    "-add ability for stratified sampling\n",
    "##########################################\n",
    "'''\n",
    "random_state = 47\n",
    "test_size = .1\n",
    "stopwords_file = 'stopwords/trademark_stopwords.txt'\n",
    "data_dir = '/home/ubuntu/multilabel-text-classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def getStopWords():\n",
    "    f = open(os.path.join(data_dir, stopwords_file))\n",
    "    stopWordsList = f.read().splitlines()\n",
    "    return stopWordsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordsList = getStopWords()\n",
    "en_stop = set(stopwords.words('english'))\n",
    "en_stop.update(['.','(','(', ')','u',',', '\"', \"'\", '?', '!', ':', ';' ,'(', ')', '[', ']', '{', '}', '&', '/', '-', '+', '*',\\\n",
    "                       \"..\", '...', '....', \"+...\", \"-...\", \",...\", \"'...\", \"!!!\", \"&...\", \"(...\", \")...\", \"]...\", \"/...\", \"(+)...\", \"),...\",\\\n",
    "                       \"),\", \").\", \"):\", \")-\", \"))\", \"])\", \".)\", \"!)\", \"')\", \"][\", '\").', '\")/', '\",', '\"-', '\")', '\"/', './', '--',\\\n",
    "                       \"#:\", \"(+)\", \"($\", \"-$\", \"/\",\"+/\",\",\", \"+-\", \"(#\", \"''\"])\n",
    "stopWords = en_stop.union(stopwordsList)\n",
    "#stopWords = en_stop\n",
    "#print (stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "##########################################\n",
    "TODO\n",
    "-Add ability to select subset of labels\n",
    "-Add ability to set data_dir (various impacts throughout files)\n",
    "##########################################\n",
    "\n",
    "data = get_trademark_data(\n",
    "    statement_url='https://bulkdata.uspto.gov/data/trademark/casefile/economics/2016/statement.csv.zip',\n",
    "    design_url='https://bulkdata.uspto.gov/data/trademark/casefile/economics/2016/design_search.csv.zip',\n",
    "    sample_size=1, #1 means 100% of the data. Lower it (.1) to run faster when testing script\n",
    "    test_size=.2, #Percentage of data to set aside for testing\n",
    "    filename='trademark_data.csv', #save retrieved data as this name, if exists will load .csv instead of url download\n",
    "    data_dir='', #save retrieved data in this folder\n",
    "    force_download=False #redownload from url and overwrite and saved .csv\n",
    "    )\n",
    "\n",
    "data.head()\n",
    "\n",
    "X,y = prepare_Xy(data)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = prepare_Xy_goodsservices_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "##########################################\n",
    "TODO\n",
    "-add analysis jupyter workbook\n",
    "\n",
    "##########################################\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(y)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "copyofX = copy.deepcopy(X)\n",
    "copyofy = copy.deepcopy(y)\n",
    "#copyofX\n",
    "#len(copyofX)\n",
    "#len(copyofy)\n",
    "#print (copyofy[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __remove_stop_words(text):\n",
    "    return ' '.join([word for word in text.split() if word.lower() not in stopWords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "myList= []\n",
    "for sentence in copyofX :\n",
    "    #print (sentence)\n",
    "    sentence= shortword.sub('', sentence)\n",
    "    sentence = __remove_stop_words(sentence)\n",
    "    #sentence=re.sub('([^\\s\\w]|_)+',' ',sentence).strip() \n",
    "    #sentence=re.sub('[?!\\\\,]+','',sentence).strip() #remove comma from sentence\n",
    "    #sentence=re.sub('[?!\\\\.]+','',sentence).strip() #remove punctuation from sentence because we don't need it anymore\n",
    "    #sentence=re.sub('[?!\\\\\"]+','',sentence).strip() #remove quotes from sentence\n",
    "    #sentence=re.sub('[?!\\\\-]+','',sentence).strip() #remove dash from sentence\n",
    "    sentence=re.sub('\\s+',' ',sentence)\t# remove multiple spaces\n",
    "    myList.append(sentence.lower())\n",
    "    #print (sentence.lower())\n",
    "print (len(myList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Transform name species into numerical values \n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)\n",
    "y = np_utils.to_categorical(y)\n",
    "#print(new_Y)\n",
    "print (y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thefile = open('test.txt', 'w')\n",
    "#for item in myList:\n",
    " # thefile.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 200\n",
    "MAX_NB_WORDS = 50000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n",
    "tokenizer.fit_on_texts(myList)\n",
    "sequences = tokenizer.texts_to_sequences(myList)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "#labels = to_categorical(np.asarray(labels)) #aaron removed\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "y = y[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = y[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = y[-num_validation_samples:]\n",
    "\n",
    "print('shape of:', x_train.shape)\n",
    "print('shape of:',y_train.shape)\n",
    "print('shape of:',x_val.shape)\n",
    "print('shape of:',y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Preparing embedding matrix.')\n",
    "#embeddings_index = {}\n",
    "# prepare embedding matrix\n",
    "#num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "#embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "#for word, i in word_index.items():\n",
    "#    if i >= MAX_NB_WORDS:\n",
    "#        continue\n",
    "#    embedding_vector = embeddings_index.get(word)\n",
    "#    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "#        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "#embedding_layer = Embedding(num_words,\n",
    "#                            EMBEDDING_DIM,\n",
    "#                            weights=[embedding_matrix],\n",
    "#                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "#                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.layers import Embedding\n",
    "\n",
    "BASE_DIR = ''\n",
    "#GLOVE_DIR = os.path.join(BASE_DIR, 'glove_embedding')\n",
    "#TEXT_DATA_DIR = os.path.join(BASE_DIR, 'glove.6B.300d.txt')\n",
    "\n",
    "\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'tm_goods_services_classification')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, 'tm_goods_services.bin')\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "MAX_NB_WORDS = 50000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'tm_goods_services.vec'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training model.')\n",
    "\n",
    "#sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "#embedded_sequences = embedding_layer(sequence_input)\n",
    "#x = Conv1D(128, 3, activation='relu')(embedded_sequences)\n",
    "#x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "#x = GlobalMaxPooling1D()(x)\n",
    "#x = Dense(50, activation=\"relu\")(x)\n",
    "#x = Dropout(0.1)(x)\n",
    "#preds = Dense(y_train.shape[1], activation='sigmoid')(x) # for multilabel classification # length of labels\n",
    "#model = Model(sequence_input, preds)\n",
    "\n",
    "#sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "#embedded_sequences = embedding_layer(sequence_input)\n",
    "#x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "#x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "#x = MaxPooling1D(5)(x)\n",
    "#x = Conv1D(128, 5, activation='relu')(x)\n",
    "#x = MaxPooling1D(35)(x)\n",
    "#x = Conv1D(128, 1, activation='relu')(x)\n",
    "#x = GlobalMaxPooling1D()(x)\n",
    "#x = Dense(128, activation='relu')(x)\n",
    "#preds = Dense(y_train.shape[1], activation='sigmoid')(x) # for multilabel classification # length of labels\n",
    "#model = Model(sequence_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigru_cnn():\n",
    "    print('Loading model.')\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    units = 128\n",
    "    conv_filters = 32\n",
    "    x = Dropout(0.2)(embedded_sequences)\n",
    "    x = Bidirectional(GRU(\n",
    "    units,\n",
    "    dropout=0.2,\n",
    "    recurrent_dropout=0.2,\n",
    "    return_sequences=True))(x)\n",
    "    x = Reshape((2 * MAX_SEQUENCE_LENGTH, units, 1))(x)\n",
    "    x = Conv2D(conv_filters, (3, 3))(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    preds = Dense(y_train.shape[1], activation='sigmoid')(x) # for multilabel classification # length of labels\n",
    "    model = Model(sequence_input, preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bigru_cnn()\n",
    "model.load_weights('/data/tm_goods_services_classification/tm_goods_services.hdf5.old')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#mlb = load_object(\"test_mlb.sav\")\n",
    "prediction_prob=.3\n",
    "#print (number.inverse_transform(new_X_test))\n",
    "y_pred_results = model.predict(x_val, verbose=0)\n",
    "#print (y_pred_results)\n",
    "y_pred = np.where (y_pred_results > prediction_prob, 1, 0)\n",
    "#print (y_pred)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print ((encoder.classes_))\n",
    "x = map(str, encoder.classes_)\n",
    "new_classes = list(x)\n",
    "print(new_classes)\n",
    "print (len (encoder.classes_))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(classification_report(y_val, y_pred)) #predict_proba scores by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (map(str, encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlb = load_object(\"test_mlb.sav\")\n",
    "'''\n",
    "print(classification_report(y_val, y_pred, target_names=new_classes)) #predict_proba scores by class\n",
    "print( classification_report( y_val, y_pred ,target_names=new_classes) , file=open('new_300d_goods_services_BI_GRU_tm_embedding_0.3.txt', 'w'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from keras.utils.np_utils import to_categorical\n",
    "    \n",
    "#test_txt = ['LEAD PENCILS, COLORED PENCILS AND COPYING INK PENCILS, INK AND PENCIL ERASERS']\n",
    "\n",
    "#88042125\n",
    "test_txt1 = ['Computer programming; Computer software consultancy; Computer software design; Creating and maintaining web sites for others; Industrial and graphic art design; Installation of computer software; Repair of computer software; Updating of computer software; Video game development services; Planning, design and implementation of computer technologies for others']\n",
    "\n",
    "test_txt1 = __remove_stop_words(test_txt1[0])\n",
    "test_txt1=re.sub('([^\\s\\w]|_)+',' ',test_txt1).strip() \n",
    "test_txt1=re.sub('\\s+',' ',test_txt1)\t# remove multiple spaces\n",
    "test_txt1 = [str (test_txt1.lower())]\n",
    "#print (test_txt1)\n",
    "#test_txt = ['stylized lion circle', 'mermaid cup vapor']\n",
    "prediction_prob=.7\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "MAX_NB_WORDS = 50000\n",
    "VALIDATION_SPLIT = 1\n",
    "#print (len(myList))\n",
    "#print (len(test_txt1))\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n",
    "tokenizer.fit_on_texts(myList)\n",
    "prediction_sequences = tokenizer.texts_to_sequences(test_txt1)\n",
    "#print (sequences)\n",
    "prediction_word_index = tokenizer.word_index\n",
    "#print('Found %s unique tokens.' % len(prediction_word_index))\n",
    "\n",
    "prediction_data = pad_sequences(prediction_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "#print (prediction_data)\n",
    "#labels = to_categorical(np.asarray(labels)) #aaron removed\n",
    "#print('Shape of data tensor:', prediction_data.shape)\n",
    "#print('Shape of label tensor:', y.shape)\n",
    "\n",
    "indices = np.arange(prediction_data.shape[0])\n",
    "#np.random.shuffle(indices)\n",
    "prediction_data = prediction_data[indices]\n",
    "prediction_y1 = y[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "prediction_x_train = prediction_data[:-num_validation_samples]\n",
    "prediction_y_train = prediction_y1[:-num_validation_samples]\n",
    "prediction_x_val = prediction_data[-num_validation_samples:]\n",
    "prediction_y_val = prediction_y1[-num_validation_samples:]\n",
    "\n",
    "#print('shape of x_train:', prediction_x_train.shape)\n",
    "#print('shape of y_train:',prediction_y_train.shape)\n",
    "#print('shape of x_val:',prediction_x_val.shape)\n",
    "#print('shape of y_val:',prediction_y_val.shape)\n",
    "\n",
    "\n",
    "test_prediction = model.predict(prediction_x_val, verbose=0)\n",
    "#print (test_prediction)\n",
    "new_test_prediction = np.where (test_prediction > prediction_prob, 1, 0)\n",
    "#print (new_test_prediction)\n",
    "new_predictions_list = [x for x in encoder.classes_[np.nonzero(new_test_prediction)[1]]]\n",
    "print (\"International class for serial no 88042125 : \", new_predictions_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
